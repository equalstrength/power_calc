
@article{anderson_sample-size_2017,
	title = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}: {A} {Method} {Adjusting} {Sample} {Effect} {Sizes} for {Publication} {Bias} and {Uncertainty}},
	volume = {28},
	issn = {0956-7976, 1467-9280},
	shorttitle = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797617723724},
	doi = {10.1177/0956797617723724},
	abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
	language = {en},
	number = {11},
	urldate = {2023-07-05},
	journal = {Psychological Science},
	author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
	month = nov,
	year = {2017},
	pages = {1547--1562},
}

@techreport{baranger_tutorial_2022,
	type = {preprint},
	title = {Tutorial: {Power} analyses for interaction effects in cross-sectional regressions},
	shorttitle = {Tutorial},
	url = {https://osf.io/5ptd7},
	abstract = {Interaction analyses (also termed ‘moderation’ analyses or ‘moderated multiple regression’) are a form of linear regression analysis designed to test whether the association between two variables changes when conditioned on a third variable. It can be challenging to perform a power analysis for interactions with existing software, particularly when variables are correlated and continuous. Moreover, while power is impacted by main effects, their correlation, and variable reliability, it can be unclear how to incorporate these effects into a power analysis. The R package InteractionPoweR and associated Shiny apps allow researchers with minimal or no programming experience to perform analytic and simulation-based power analyses for interactions. At minimum, these analyses require the Pearson’s correlation between variables and sample size, and additional parameters including reliability and the number of discrete levels that a variable takes (e.g., binary or likert scale) can optionally be specified. In this Tutorial we demonstrate how to perform power analyses using our package and give examples of how power can be impacted by main effects, correlations between main effects, reliability, and variable distributions. We also include a brief discussion of how researchers may select an appropriate interaction effect size when performing a power analysis.},
	urldate = {2023-07-04},
	institution = {PsyArXiv},
	author = {Baranger, David A and Finsaas, Megan and Goldstein, Brandon and Vize, Colin and Lynam, Donald and Olino, Thomas M},
	month = aug,
	year = {2022},
	doi = {10.31234/osf.io/5ptd7},
	file = {Submitted Version:/Users/daniel/Zotero/storage/F368T9LH/Baranger et al. - 2022 - Tutorial Power analyses for interaction effects i.pdf:application/pdf},
}

@book{bulus_pwrss_2023,
	title = {pwrss: {Statistical} {Power} and {Sample} {Size} {Calculation} {Tools}},
	url = {https://CRAN.R-project.org/package=pwrss},
	author = {Bulus, Metin},
	year = {2023},
}

@book{maxwell_designing_2017,
	address = {New York, NY},
	edition = {Third edition},
	title = {Designing experiments and analyzing data: a model comparison perspective},
	isbn = {978-1-138-89228-6},
	shorttitle = {Designing experiments and analyzing data},
	publisher = {Routledge},
	author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
	year = {2017},
	keywords = {Experimental design},
}

@article{gomila_logistic_2021,
	title = {Logistic or linear? {Estimating} causal effects of experimental treatments on binary outcomes using regression analysis.},
	volume = {150},
	issn = {1939-2222, 0096-3445},
	shorttitle = {Logistic or linear?},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000920},
	doi = {10.1037/xge0000920},
	language = {en},
	number = {4},
	urldate = {2023-07-04},
	journal = {Journal of Experimental Psychology: General},
	author = {Gomila, Robin},
	month = apr,
	year = {2021},
	pages = {700--709},
	file = {Submitted Version:/Users/daniel/Zotero/storage/X5L6WIPV/Gomila - 2021 - Logistic or linear Estimating causal effects of e.pdf:application/pdf},
}

@incollection{cohen_concepts_1977,
	title = {The {Concepts} of {Power} {Analysis}},
	isbn = {978-0-12-179060-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780121790608500062},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Statistical {Power} {Analysis} for the {Behavioral} {Sciences}},
	publisher = {Elsevier},
	author = {Cohen, Jacob},
	year = {1977},
	doi = {10.1016/B978-0-12-179060-8.50006-2},
	pages = {1--17},
	file = {Cohen - 1977 - The Concepts of Power Analysis.pdf:/Users/daniel/Zotero/storage/QPAGXTMH/Cohen - 1977 - The Concepts of Power Analysis.pdf:application/pdf},
}

@article{mcshane_abandon_2019,
	title = {Abandon {Statistical} {Significance}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	number = {sup1},
	urldate = {2023-07-04},
	journal = {The American Statistician},
	author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1527253},
	keywords = {Null hypothesis significance testing, p-Value, Replication, Sociology of science, Statistical significance},
	pages = {235--245},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/WMU8V9NN/McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf},
}

@article{gelman_beyond_2014,
	title = {Beyond {Power} {Calculations}: {Assessing} {Type} {S} ({Sign}) and {Type} {M} ({Magnitude}) {Errors}},
	volume = {9},
	issn = {1745-6916},
	shorttitle = {Beyond {Power} {Calculations}},
	url = {https://doi.org/10.1177/1745691614551642},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2023-07-04},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {641--651},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/GCX4NPDG/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf:application/pdf},
}

@article{champely_pwr_2017,
	title = {pwr: {Basic} functions for power analysis},
	author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and De Rosario, Helios},
	year = {2017},
}

@book{gelman_regression_2020,
	edition = {1},
	title = {Regression and {Other} {Stories}},
	isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0},
	url = {https://www.cambridge.org/highereducation/product/9781139161879/book},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	urldate = {2023-07-05},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	month = jul,
	year = {2020},
	doi = {10.1017/9781139161879},
}

@article{ramos_labour_2021,
	title = {Labour market discrimination against {Moroccan} minorities in the {Netherlands} and {Spain}: a cross-national and cross-regional comparison},
	volume = {47},
	issn = {1369-183X},
	shorttitle = {Labour market discrimination against {Moroccan} minorities in the {Netherlands} and {Spain}},
	url = {https://doi.org/10.1080/1369183X.2019.1622824},
	doi = {10.1080/1369183X.2019.1622824},
	abstract = {This paper examines discrimination against job candidates of Moroccan origin in Spain and the Netherlands. By drawing on insights from group threat theory, we specifically examine how latent ethnic conflicts regarding economic goods or cultural values at the regional level affect discrimination rates of Moroccan minorities in both countries. To this aim, we make use of a cross-nationally standardised field experiment with fictitious candidates applying for real job vacancies in Spain and the Netherlands (n = 3681). We find higher levels of discrimination against job applicants of Moroccan origin in the Netherlands. Whereas job candidates of Moroccan origin are six percentage points less likely to receive a positive response from an employer in Spain, the predicted ethnic gap in call-back rates is fourteen percentage points in the Netherlands. Furthermore, while regional differences in discrimination are not related to regional unemployment figures, we do find some evidence that in the Netherlands a larger share of Moroccans in the region exacerbates discrimination against Moroccan minorities. Altogether, the findings point to the need to give greater weight to the impact of widespread negative beliefs about ethnic minorities and how these beliefs can have a profound adverse impact on the integration of disadvantaged ethnic groups within the labour market.},
	number = {6},
	urldate = {2023-07-10},
	journal = {Journal of Ethnic and Migration Studies},
	author = {Ramos, María and Thijssen, Lex and Coenders, Marcel},
	month = apr,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1369183X.2019.1622824},
	keywords = {Comparative research, correspondence testing, ethnic discrimination, group threat theory, Moroccan minorities},
	pages = {1261--1284},
}

@article{larsen_pakistani_2021,
	title = {Pakistani in the {UK} and {Norway}: different contexts, similar disadvantage. {Results} from a comparative field experiment on hiring discrimination},
	volume = {47},
	issn = {1369-183X},
	shorttitle = {Pakistani in the {UK} and {Norway}},
	url = {https://doi.org/10.1080/1369183X.2019.1622777},
	doi = {10.1080/1369183X.2019.1622777},
	abstract = {Experimental research on ethnic discrimination in labour markets is often concerned with individual-level mechanisms. Structural macro-level drivers of discrimination, however, are more difficult to address. We present the results of a harmonised correspondence test conducted in Norway and the UK, two contexts that differ significantly along several dimensions expected to matter for the prevalence of ethnic discrimination. We focus on discrimination towards a particular minority group that holds a socio-economically disadvantaged position in both national contexts: Pakistani migrants and their descendants. Based on differences in labour market flexibility and anti-discrimination policies, we expect there a lower level of discrimination than in Norway. In addition, colonial ties and a longer history of immigration in Britain could potentially reduce discriminatory behaviour towards ethnic minorities in general. We show that in both countries Pakistani applicants receive significantly fewer positive responses from employers than the majority group. In line with expectations, discrimination is more severe in Norway, although this cross-national difference is only statistically significant when using a strict definition of positive callbacks. More nuanced analyses reveal an additional negative effect of disclosing one’s affiliation to Islam, which further reduces the likelihood of Pakistani applicants to receive a callback in Norway.},
	number = {6},
	urldate = {2023-07-10},
	journal = {Journal of Ethnic and Migration Studies},
	author = {Larsen, Edvard N. and Di Stasio, Valentina},
	month = apr,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1369183X.2019.1622777},
	keywords = {correspondence test, employers, Ethnic discrimination, field experiment, hiring, Muslim, Pakistani},
	pages = {1201--1221},
	file = {Accepted Version:/Users/daniel/Zotero/storage/RJ9JL5YX/Larsen and Di Stasio - 2021 - Pakistani in the UK and Norway different contexts.pdf:application/pdf},
}

@article{lakens_sample_2022,
	title = {Sample {Size} {Justification}},
	volume = {8},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.33267},
	doi = {10.1525/collabra.33267},
	abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
	number = {1},
	urldate = {2023-07-10},
	journal = {Collabra: Psychology},
	author = {Lakens, Daniël},
	month = mar,
	year = {2022},
	pages = {33267},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/SCJRZSJH/Lakens - 2022 - Sample Size Justification.pdf:application/pdf;Snapshot:/Users/daniel/Zotero/storage/BCMJ9RMW/Sample-Size-Justification.html:text/html},
}
