
@article{anderson_sample-size_2017,
	title = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}: {A} {Method} {Adjusting} {Sample} {Effect} {Sizes} for {Publication} {Bias} and {Uncertainty}},
	volume = {28},
	issn = {0956-7976, 1467-9280},
	shorttitle = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797617723724},
	doi = {10.1177/0956797617723724},
	abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
	language = {en},
	number = {11},
	urldate = {2023-07-05},
	journal = {Psychological Science},
	author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
	month = nov,
	year = {2017},
	pages = {1547--1562},
}

@techreport{baranger_tutorial_2022,
	type = {preprint},
	title = {Tutorial: {Power} analyses for interaction effects in cross-sectional regressions},
	shorttitle = {Tutorial},
	url = {https://osf.io/5ptd7},
	abstract = {Interaction analyses (also termed ‘moderation’ analyses or ‘moderated multiple regression’) are a form of linear regression analysis designed to test whether the association between two variables changes when conditioned on a third variable. It can be challenging to perform a power analysis for interactions with existing software, particularly when variables are correlated and continuous. Moreover, while power is impacted by main effects, their correlation, and variable reliability, it can be unclear how to incorporate these effects into a power analysis. The R package InteractionPoweR and associated Shiny apps allow researchers with minimal or no programming experience to perform analytic and simulation-based power analyses for interactions. At minimum, these analyses require the Pearson’s correlation between variables and sample size, and additional parameters including reliability and the number of discrete levels that a variable takes (e.g., binary or likert scale) can optionally be specified. In this Tutorial we demonstrate how to perform power analyses using our package and give examples of how power can be impacted by main effects, correlations between main effects, reliability, and variable distributions. We also include a brief discussion of how researchers may select an appropriate interaction effect size when performing a power analysis.},
	urldate = {2023-07-04},
	institution = {PsyArXiv},
	author = {Baranger, David A and Finsaas, Megan and Goldstein, Brandon and Vize, Colin and Lynam, Donald and Olino, Thomas M},
	month = aug,
	year = {2022},
	doi = {10.31234/osf.io/5ptd7},
	file = {Submitted Version:/Users/daniel/Zotero/storage/F368T9LH/Baranger et al. - 2022 - Tutorial Power analyses for interaction effects i.pdf:application/pdf},
}

@book{bulus_pwrss_2023,
	title = {pwrss: {Statistical} {Power} and {Sample} {Size} {Calculation} {Tools}},
	url = {https://CRAN.R-project.org/package=pwrss},
	author = {Bulus, Metin},
	year = {2023},
}

@book{maxwell_designing_2017,
	address = {New York, NY},
	edition = {Third edition},
	title = {Designing experiments and analyzing data: a model comparison perspective},
	isbn = {978-1-138-89228-6},
	shorttitle = {Designing experiments and analyzing data},
	publisher = {Routledge},
	author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
	year = {2017},
	keywords = {Experimental design},
}

@article{gomila_logistic_2021,
	title = {Logistic or linear? {Estimating} causal effects of experimental treatments on binary outcomes using regression analysis.},
	volume = {150},
	issn = {1939-2222, 0096-3445},
	shorttitle = {Logistic or linear?},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000920},
	doi = {10.1037/xge0000920},
	language = {en},
	number = {4},
	urldate = {2023-07-04},
	journal = {Journal of Experimental Psychology: General},
	author = {Gomila, Robin},
	month = apr,
	year = {2021},
	pages = {700--709},
	file = {Submitted Version:/Users/daniel/Zotero/storage/X5L6WIPV/Gomila - 2021 - Logistic or linear Estimating causal effects of e.pdf:application/pdf},
}

@incollection{cohen_concepts_1977,
	title = {The {Concepts} of {Power} {Analysis}},
	isbn = {978-0-12-179060-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780121790608500062},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Statistical {Power} {Analysis} for the {Behavioral} {Sciences}},
	publisher = {Elsevier},
	author = {Cohen, Jacob},
	year = {1977},
	doi = {10.1016/B978-0-12-179060-8.50006-2},
	pages = {1--17},
	file = {Cohen - 1977 - The Concepts of Power Analysis.pdf:/Users/daniel/Zotero/storage/QPAGXTMH/Cohen - 1977 - The Concepts of Power Analysis.pdf:application/pdf},
}

@article{mcshane_abandon_2019,
	title = {Abandon {Statistical} {Significance}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	number = {sup1},
	urldate = {2023-07-04},
	journal = {The American Statistician},
	author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1527253},
	keywords = {Null hypothesis significance testing, p-Value, Replication, Sociology of science, Statistical significance},
	pages = {235--245},
	file = {Full Text PDF:/Users/daniel/Zotero/storage/WMU8V9NN/McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf},
}

@article{gelman_beyond_2014,
	title = {Beyond {Power} {Calculations}: {Assessing} {Type} {S} ({Sign}) and {Type} {M} ({Magnitude}) {Errors}},
	volume = {9},
	issn = {1745-6916},
	shorttitle = {Beyond {Power} {Calculations}},
	url = {https://doi.org/10.1177/1745691614551642},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2023-07-04},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {641--651},
	file = {SAGE PDF Full Text:/Users/daniel/Zotero/storage/GCX4NPDG/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf:application/pdf},
}

@article{champely_pwr_2017,
	title = {pwr: {Basic} functions for power analysis},
	author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and De Rosario, Helios},
	year = {2017},
}

@book{gelman_regression_2020,
	edition = {1},
	title = {Regression and {Other} {Stories}},
	isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0},
	url = {https://www.cambridge.org/highereducation/product/9781139161879/book},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	urldate = {2023-07-05},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	month = jul,
	year = {2020},
	doi = {10.1017/9781139161879},
}
